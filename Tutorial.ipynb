{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lakh MIDI Dataset Tutorial\n",
    "\n",
    "This IPython notebook demonstrates how to take advantage of the data in the [Lakh MIDI Dataset](http://colinraffel.com/projects/lmd/).  Specifically, it shows how the dataset is organized and demonstrates possibilities for utilizing annotations extracted from LMD-aligned (i.e., the collection of MIDI files which have been matched and aligned to entries in the MSD).  We will be using [pretty_midi](https://github.com/craffel/pretty-midi) for parsing the MIDI files, [mir_eval](https://github.com/craffel/mir_eval) for sonification and visualization, and [librosa](https://github.com/librosa/librosa) for audio analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pretty_midi\n",
    "import librosa\n",
    "import mir_eval\n",
    "import mir_eval.display\n",
    "import tables\n",
    "import IPython.display\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Local path constants\n",
    "DATA_PATH = 'data'\n",
    "RESULTS_PATH = 'results'\n",
    "# Path to the file unique_midi_match_scores.js distributed with the LMD\n",
    "SCORE_FILE = os.path.join(RESULTS_PATH, 'unique_midi_match_scores.js')\n",
    "# Path to where the Million Song Dataset h5 files live\n",
    "MSD_PATH = '/media/hdd1/data/MillionSong/data/'\n",
    "\n",
    "\n",
    "# Utility functions for retrieving paths\n",
    "def msd_id_to_dirs(msd_id):\n",
    "    \"\"\"Given an MSD ID, generate the path prefix.\n",
    "    E.g. TRABCD12345678 -> A/B/C/TRABCD12345678\"\"\"\n",
    "    return os.path.join(msd_id[2], msd_id[3], msd_id[4], msd_id)\n",
    "\n",
    "def msd_id_to_mp3(msd_id):\n",
    "    \"\"\"Given an MSD ID, return the path to the corresponding mp3\"\"\"\n",
    "    return os.path.join(DATA_PATH, 'msd', 'mp3',\n",
    "                        msd_id_to_dirs(msd_id) + '.mp3')\n",
    "\n",
    "def msd_id_to_h5(h5):\n",
    "    \"\"\"Given an MSD ID, return the path to the corresponding h5\"\"\"\n",
    "    return os.path.join(MSD_PATH, msd_id_to_dirs(msd_id) + '.h5')\n",
    "\n",
    "def get_midi_path(msd_id, midi_md5, kind):\n",
    "    \"\"\"Given an MSD ID and MIDI MD5, return path to a MIDI file.\n",
    "    kind should be one of 'aligned' or 'unaligned'. \"\"\"\n",
    "    return os.path.join(RESULTS_PATH, 'unique_midi_matched_{}'.format(kind),\n",
    "                        msd_id_to_dirs(msd_id), midi_md5 + '.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data layout\n",
    "\n",
    "The file [`unique_midi_match_scores.js`](http://colinraffel.com/projects/lmd/unique_midi_match_scores.js) is a good place to start.  It holds a dictionary of dictionaries.  The first dictionary is keyed by Million Song Dataset IDs.  Each entry in this dictionary is a dictionary which maps MD5 checksums of MIDI files in the Lakh MIDI Dataset to scores in the range `[0.5, 1.0]` which represent the confidence that a given MIDI file matches a given Million Song Dataset entry.  This strange range of confidence scores is due to the fact that below `0.5` it's likely that the match is invalid.  Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(SCORE_FILE) as f:\n",
    "    scores = json.load(f)\n",
    "# Grab a Million Song Dataset ID from the scores dictionary\n",
    "msd_id = scores.keys()[1234]\n",
    "print 'Million Song Dataset ID {} has {} MIDI file matches:'.format(\n",
    "    msd_id, len(scores[msd_id]))\n",
    "for midi_md5, score in scores[msd_id].items():\n",
    "    print '  {} with confidence score {}'.format(midi_md5, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this Million Song Dataset entry has xxxx MIDI files matched to it, with scores xxxx.  That there are multiple MIDI files matched to this one Million Song Dataset entry means that the Lakh MIDI Dataset has multiple _different_ MIDI transcriptions of this single piece of music.\n",
    "\n",
    "MIDI files which have been matched to the Million Song Dataset are distributed in the Lakh MIDI Dataset in two formats: First, in [`unique_midi_unaligned.tar.gz`](http://colinraffel.com/projects/lmd/unique_midi_unaligned.tar.gz), in their raw form as they were scraped from the internet; and second, in [`unique_midi_aligned.tar.gz`](http://colinraffel.com/projects/lmd/unique_midi_unaligned.tar.gz), they are pre-aligned to the 7digital preview clips which accompany the Million Song Dataset.  The directory structure of both of these packages follows the Million Song Dataset.  For example, the MIDI files we just inspected above appear in\n",
    "```\n",
    "N/C/Z/TRNCZVX128F92F9018/a92af10c0349706ba12552011f7f77a8.mid\n",
    "N/C/Z/TRNCZVX128F92F9018/335a5edca8882f4d2725683d7c530aac.mid\n",
    "N/C/Z/TRNCZVX128F92F9018/8e2bbe4485b113ba48762b1e3032795a.mid\n",
    "N/C/Z/TRNCZVX128F92F9018/df21ff6afbeab449e4d415167c54decf.mid\n",
    "N/C/Z/TRNCZVX128F92F9018/22a0142b14b393b1515062d8a006814e.mid\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing aligned MIDI files\n",
    "\n",
    "MIDI files provide a wide variety of useful information.  Moreover, when they are matched and aligned to audio recordings, they can be used to derive annotations about the recording.  Below is a demonstration of how to extract this information from the files in LMD-aligned.  We'll start by grabbing a MIDI file which has all of the useful information we will demonstrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Grab an MSD ID its matches dictionary\n",
    "    msd_id, matches = scores.popitem()\n",
    "    # Grab a MIDI from the matches\n",
    "    midi_md5, score = matches.popitem()\n",
    "    # Construct the path to the aligned MIDI\n",
    "    aligned_midi_path = get_midi_path(msd_id, midi_md5, 'aligned')\n",
    "    # Load/parse the MIDI file with pretty_midi\n",
    "    pm = pretty_midi.PrettyMIDI(aligned_midi_path)\n",
    "    # Look for a MIDI file which has lyric and key signature change events\n",
    "    if len(pm.lyrics) > 5 and len(pm.key_signature_changes) > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MIDI files in LMD-aligned are pre-aligned to 7digital preview clips from the MSD\n",
    "# Let's listen to this aligned MIDI along with its preview clip\n",
    "# Load in the audio data\n",
    "audio, fs = librosa.load(msd_id_to_mp3(msd_id))\n",
    "# Synthesie the audio using fluidsynth\n",
    "midi_audio = pm.fluidsynth(fs)\n",
    "# Play audio in one channel, synthesized MIDI in the other\n",
    "IPython.display.Audio([audio, midi_audio[:audio.shape[0]]], rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata from the MSD\n",
    "\n",
    "We can use information from the h5 file for the Million Song Dataset entry to determine metadata about this MIDI file match.  The MSD also has additional useful information, like terms which describe the artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tables.open_file(msd_id_to_h5(msd_id)) as h5:\n",
    "    print 'ID: {}'.format(msd_id)\n",
    "    print '\"{}\" by {} on \"{}\"'.format(\n",
    "        h5.root.metadata.songs.cols.title[0],\n",
    "        h5.root.metadata.songs.cols.artist_name[0],\n",
    "        h5.root.metadata.songs.cols.release[0])\n",
    "    print 'Top 5 artist terms:', ', '.join(list(h5.root.metadata.artist_terms)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcription\n",
    "\n",
    "MIDI files are at their core a way of storing a score of a piece of music, so the MIDI files in the Lakh MIDI dataset can be used to obtain a transcription of the audio files they are matched and aligned to.  `pretty_midi`'s `get_piano_roll()` method provides a convenient way of getting a \"piano roll representation\" of the transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve piano roll of the MIDI file\n",
    "piano_roll = pm.get_piano_roll()\n",
    "# Use 7 octaves starting from C1\n",
    "piano_roll = piano_roll[12:96]\n",
    "# Retrieve the audio corresponding to this MSD entry\n",
    "audio, fs = librosa.load(msd_id_to_mp3(msd_id))\n",
    "# Compute constant-Q spectrogram\n",
    "cqt = librosa.logamplitude(librosa.cqt(audio))\n",
    "# Normalize for visualization\n",
    "cqt = librosa.util.normalize(cqt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(211)\n",
    "librosa.display.specshow(piano_roll, y_axis='cqt_note', cmap=plt.cm.hot)\n",
    "plt.title('MIDI piano roll')\n",
    "plt.subplot(212)\n",
    "librosa.display.specshow(cqt, y_axis='cqt_note', x_axis='time',\n",
    "                         cmap=plt.cm.hot, vmin=np.percentile(cqt, 25))\n",
    "plt.title('Audio CQT');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MIDI files, different instruments are transcribed on different tracks or channels, which allows them to be separated easily.  You can therefore easily get per-instrument piano rolls with `pretty_midi`.  This enables the potential of creating annotations for melody extraction and score-informed source separation.  Note that while methods exist for giving instruments arbitrary text names, in practice are limited to the instruments in the General MIDI specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve piano roll of one of the instruments\n",
    "piano_roll = pm.instruments[4].get_piano_roll()\n",
    "piano_roll = piano_roll[12:96]\n",
    "plt.figure(figsize=(10, 3))\n",
    "librosa.display.specshow(piano_roll, y_axis='cqt_note', cmap=plt.cm.hot)\n",
    "# Get the text name of this instrument's program number\n",
    "program_name = pretty_midi.program_to_instrument_name(pm.instruments[4].program)\n",
    "plt.title('Instrument 4 ({}) piano roll'.format(program_name));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pretty_midi also provides direct access to the pitch and start/end time of each note\n",
    "intervals = np.array([[note.start, note.end] for note in pm.instruments[4].notes])\n",
    "notes = np.array([note.pitch for note in pm.instruments[4].notes])\n",
    "plt.figure(figsize=(10, 3))\n",
    "mir_eval.display.piano_roll(intervals, midi=notes, facecolor='orange')\n",
    "plt.title('Instrument 4 ({}) piano roll'.format(program_name))\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('MIDI note number');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meter\n",
    "\n",
    "Because MIDI files are scores, they contain full meter information (e.g. time signatures, beats, and downbeats).  Since the MIDI files in LMD-aligned are pre-aligned to corresponding 7digital preview clips in the MSD, we can extract beat and downbeat annotations for the 7digital preview clip from the MIDI file directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve the beats and downbeats from pretty_midi\n",
    "# Note that the beat phase will be wrong until the first time signature change after 0s\n",
    "# So, let's start beat tracking from that point\n",
    "first_ts_after_0 = [ts.time for ts in pm.time_signature_changes if ts.time > 0.][0]\n",
    "# Get beats from pretty_midi, supplying a start time\n",
    "beats = pm.get_beats(start_time=first_ts_after_0)\n",
    "# .. downbeats, too\n",
    "downbeats = pm.get_downbeats(start_time=first_ts_after_0)\n",
    "# Display meter on top of waveform\n",
    "plt.figure(figsize=(10, 3))\n",
    "librosa.display.waveplot(audio, color='green', alpha=.5)\n",
    "mir_eval.display.events(beats, base=-1, height=2, color='orange')\n",
    "mir_eval.display.events(downbeats, base=-1, height=2, color='black', lw=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Synthesize clicks at these downbeat times\n",
    "beat_clicks = librosa.clicks(beats, length=audio.shape[0])\n",
    "downbeat_clicks = librosa.clicks(downbeats, click_freq=2000, length=audio.shape[0])\n",
    "IPython.display.Audio([audio, beat_clicks + downbeat_clicks], rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key changes\n",
    "\n",
    "MIDI files can contain key signature annotations.  They are, however, a completely optional meta-event, so many MIDI files don't have one.  Because key changes are rare in Western popular music, most MIDI files which do have key signature change annotations only have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print out all key changes in the MIDI file\n",
    "for key_change in pm.key_signature_changes:\n",
    "    print 'Key {} starting at time {:.2f}'.format(\n",
    "        pretty_midi.key_number_to_key_name(key_change.key_number), key_change.time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lyrics\n",
    "\n",
    "MIDI files can also optionally have lyrics events, which are essentially timestamped text.  Lyrics are often transcribed at the syllable level, but also occasionally at the character or word level.  As with meter information, because the MIDI files in LMD-aligned are matched and aligned to 7digital preview MP3s, we can use the MIDI files to obtain timestamped lyrics transcriptions for these recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the boundaries of each line in the lyrics\n",
    "lines = [0] + [n for n, lyric in enumerate(pm.lyrics) if '\\r' in lyric.text]\n",
    "for start, end in zip(lines[:-1], lines[1:]):\n",
    "    # Print the times of each lyric in the line, delimited by |\n",
    "    print '|'.join('{:>8.3f}'.format(lyric.time)\n",
    "                   for lyric in pm.lyrics[start:end]\n",
    "                   if lyric.text != '\\r')\n",
    "    # Print the text of each lyric in the line, delimited by |\n",
    "    print '|'.join('{:>8}'.format(lyric.text)\n",
    "                   for lyric in pm.lyrics[start:end]\n",
    "                   if lyric.text != '\\r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
